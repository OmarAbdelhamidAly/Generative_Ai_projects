{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1930c7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Loads variables from .env into os.environ\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "LANGCHAIN_API_KEY = os.getenv(\"LANGCHAIN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a98c8522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import pandas as pd\n",
    "\n",
    "# Load tokenizer and model once\n",
    "trained_tokenizer = AutoTokenizer.from_pretrained(\"tokenizer_Generative_qa_model\")\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\").to(\"cuda\")\n",
    "base_model.resize_token_embeddings(len(trained_tokenizer))\n",
    "trained_model = PeftModel.from_pretrained(base_model, \"trainer_Generative_qa_model\").to(\"cuda\")\n",
    "\n",
    "# Core generation function\n",
    "def generate_answer(question, context, model, tokenizer):\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=128,\n",
    "        min_length=20,\n",
    "        num_beams=5,\n",
    "        repetition_penalty=1.2,\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer.strip()\n",
    "\n",
    "#  Function to try on natural question\n",
    "def answer_natural_question(question, context=None, fallback_csv=\"single_qna.csv\"):\n",
    "    if context is None:\n",
    "        # If no context is provided, get a similar one from CSV randomly (you can plug in retrieval here)\n",
    "        df = pd.read_csv(fallback_csv)[['Question', 'Answer']].dropna().sample(1, random_state=42)\n",
    "        context = df.iloc[0]['Answer']\n",
    "    \n",
    "    generated = generate_answer(question, context, trained_model, trained_tokenizer)\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"used_context\": context,\n",
    "        \"generated_answer\": generated\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96d71556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_65340\\2380448054.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  question_encoder.load_state_dict(torch.load(\"q_encoder_finetuned.pth\", map_location=device))\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_65340\\2380448054.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  answer_encoder.load_state_dict(torch.load(\"a_encoder_finetuned.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "➡️ Generating and saving answer embeddings...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "# Define DualEncoder model\n",
    "class DualEncoder(nn.Module):\n",
    "    def __init__(self, model_name, output_dim=128):\n",
    "        super().__init__()\n",
    "        self.base_model = AutoModel.from_pretrained(model_name)\n",
    "        self.projection = nn.Linear(self.base_model.config.hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_token = output.last_hidden_state[:, 0]\n",
    "        return self.projection(cls_token)\n",
    "\n",
    "# Init\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load models\n",
    "question_encoder = DualEncoder(model_name).to(device)\n",
    "answer_encoder = DualEncoder(model_name).to(device)\n",
    "question_encoder.load_state_dict(torch.load(\"q_encoder_finetuned.pth\", map_location=device))\n",
    "answer_encoder.load_state_dict(torch.load(\"a_encoder_finetuned.pth\", map_location=device))\n",
    "question_encoder.eval()\n",
    "answer_encoder.eval()\n",
    "\n",
    "# Load QnA dataset\n",
    "df = pd.read_csv(\"single_qna.csv\")[['Question', 'Answer']].dropna()\n",
    "df = df.sample(n=100000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save DataFrame to reuse index mapping\n",
    "df.to_parquet(\"qna_df.parquet\", index=False)\n",
    "\n",
    "# Function to embed in batches\n",
    "def get_embeddings(model, texts, batch_size=256):\n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        tokens = tokenizer(batch_texts, padding=True, truncation=True, return_tensors='pt', max_length=64)\n",
    "        tokens.pop(\"token_type_ids\", None)\n",
    "        tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "        with torch.no_grad():\n",
    "            emb = model(**tokens)\n",
    "        all_embeddings.append(emb.cpu())\n",
    "    return torch.cat(all_embeddings, dim=0)  # shape [N, 128]\n",
    "\n",
    "# Check if saved\n",
    "embedding_path = \"answer_embeddings.pt\"\n",
    "if not os.path.exists(embedding_path):\n",
    "    print(\"➡️ Generating and saving answer embeddings...\")\n",
    "    answer_embeddings = get_embeddings(answer_encoder, df['Answer'].tolist())\n",
    "    torch.save(answer_embeddings, embedding_path)\n",
    "else:\n",
    "    print(\"✅ Loading precomputed embeddings...\")\n",
    "    answer_embeddings = torch.load(embedding_path).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83a2d441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Get top K answers\n",
    "def get_top_k_answers_dual_encoder(question, top_k=3):\n",
    "    q_emb = get_embeddings(question_encoder, [question])  # shape [1, 128]\n",
    "    sims = torch.matmul(q_emb, answer_embeddings.T).squeeze()  # [N]\n",
    "    topk_scores, topk_indices = torch.topk(sims, k=top_k)\n",
    "\n",
    "    df = pd.read_parquet(\"qna_df.parquet\")  # load cached mapping\n",
    "    top_answers = []\n",
    "    for score, idx in zip(topk_scores.tolist(), topk_indices.tolist()):\n",
    "        top_answers.append({\n",
    "            \"answer\": df.loc[idx, 'Answer'],\n",
    "            \"score\": round(score, 4),\n",
    "            \"reference_question\": df.loc[idx, 'Question']\n",
    "        })\n",
    "    \n",
    "    return top_answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3167630d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5889eaff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e000a3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "def improve_question_func(state: dict) -> dict:\n",
    "    question = state[\"question\"]\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=\"llama3-70b-8192\",\n",
    "        messages=[{\"role\": \"user\", \"content\": f\"Improve this e-commerce question: {question}\"}]\n",
    "    )\n",
    "    improved = response.choices[0].message.content.strip()\n",
    "    return {\"improved_question\": improved}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e9d7601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_dual_encoder(state: dict) -> dict:\n",
    "    improved_question = state[\"improved_question\"]\n",
    "    top_answers = get_top_k_answers_dual_encoder(improved_question, top_k=3)\n",
    "    return {\"dual_encoder_answers\": top_answers}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741ba3a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "59254b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_answer(state: dict) -> dict:\n",
    "    import re\n",
    "\n",
    "    def sanitize_text(text: str) -> str:\n",
    "        text = text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
    "        text = re.sub(r'[^\\x20-\\x7E]', '', text)\n",
    "        return text.strip()\n",
    "\n",
    "    question = state.get(\"question\", \"\")\n",
    "    dual_encoder_answers = state.get(\"dual_encoder_answers\", [])\n",
    "\n",
    "    # Format dual encoder answers as subtle hints (not context)\n",
    "    examples_parts = []\n",
    "    for ans in dual_encoder_answers:\n",
    "        ref_q = sanitize_text(ans.get('reference_question', ''))\n",
    "        ans_text = sanitize_text(ans.get('answer', ''))\n",
    "        examples_parts.append(f\"Q: {ref_q}\\nA: {ans_text}\")\n",
    "\n",
    "    examples_section = \"\\n\\n\".join(examples_parts)\n",
    "\n",
    "    print(\"==[ DEBUG: Similar QA examples (hint only) ]==\")\n",
    "    print(examples_section)\n",
    "\n",
    "    # Get initial answer from fine-tuned model\n",
    "    fine_tuned_result = answer_natural_question(question, context=\"\")  # No direct context\n",
    "    fine_tuned_answer = fine_tuned_result.get('generated_answer', '')\n",
    "\n",
    "    # Compose final instruction prompt\n",
    "    prompt = (\n",
    "        \"You are a knowledgeable and helpful assistant.\\n\\n\"\n",
    "        \"Below are some Q&A pairs from similar questions. These are **not answers to the current question**, \"\n",
    "        \"but they may help you understand the tone, style, or approach to take.\\n\\n\"\n",
    "        f\"{examples_section}\\n\\n\"\n",
    "        f\"The user asked:\\n{sanitize_text(question)}\\n\\n\"\n",
    "        f\"The initial answer was:\\n{sanitize_text(fine_tuned_answer)}\\n\\n\"\n",
    "        \"Please rewrite and improve the answer above, making it clearer, more helpful, and aligned with good examples.\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful e-commerce assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    # Call LLM (e.g., Groq, OpenAI)\n",
    "    response = groq_client.chat.completions.create(\n",
    "        model=\"llama3-70b-8192\",\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    grok_answer = response.choices[0].message.content.strip()\n",
    "\n",
    "    return {\"final_answer\": grok_answer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "243a5c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Dict, Any\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Define the state schema WITHOUT product_context\n",
    "class QAState(TypedDict):\n",
    "    question: str\n",
    "    improved_question: str\n",
    "    dual_encoder_answers: List[Dict[str, Any]]  # clearer typing\n",
    "    final_answer: str\n",
    "\n",
    "# Initialize the graph with the updated state schema\n",
    "graph = StateGraph(state_schema=QAState)\n",
    "\n",
    "# Register processing nodes WITHOUT RetrieveProductContext\n",
    "graph.add_node(\"ImproveQuestion\", RunnableLambda(improve_question_func))\n",
    "graph.add_node(\"RetrieveDualEncoder\", RunnableLambda(retrieve_dual_encoder))\n",
    "graph.add_node(\"GenerateFinalAnswer\", RunnableLambda(generate_final_answer))\n",
    "\n",
    "# Define control flow (edges between nodes) WITHOUT RetrieveProductContext\n",
    "graph.set_entry_point(\"ImproveQuestion\")\n",
    "graph.add_edge(\"ImproveQuestion\", \"RetrieveDualEncoder\")\n",
    "graph.add_edge(\"RetrieveDualEncoder\", \"GenerateFinalAnswer\")\n",
    "\n",
    "# Define the final node\n",
    "graph.add_edge(\"GenerateFinalAnswer\", END)\n",
    "\n",
    "# Compile the graph for execution\n",
    "qa_graph = graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b9657c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     +-----------+       \n",
      "     | __start__ |       \n",
      "     +-----------+       \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "  +-----------------+    \n",
      "  | ImproveQuestion |    \n",
      "  +-----------------+    \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "+---------------------+  \n",
      "| RetrieveDualEncoder |  \n",
      "+---------------------+  \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "+---------------------+  \n",
      "| GenerateFinalAnswer |  \n",
      "+---------------------+  \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "      +---------+        \n",
      "      | __end__ |        \n",
      "      +---------+        \n"
     ]
    }
   ],
   "source": [
    "print(qa_graph.get_graph().draw_ascii())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "db6d151f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==[ DEBUG: Similar QA examples (hint only) ]==\n",
      "Q: What size stoppers? Thanks\n",
      "A: I think I used a 46-50mm foam stopper\n",
      "\n",
      "Q: will a 1 1/5\" pvc pipe fit into this flange\n",
      "A: DID you mean 1 1/2\"...? I'll check and the mounts are made so you can run a pipe through a hole drilled into the decking\n",
      "\n",
      "Q: Anyone know what the screw sizes are for this liftgate support? I recently installed mine, but lost a screw in the process.\n",
      "A: If you bring the other screw in for the other side to lowe's, they will know. If that doesn't help just take in the lift itself and they should be able to gauge the size. hope this helps!\n",
      "Final Answer: I think I can help you with that!\n",
      "\n",
      "Since you have a 9-year-old Badger 1 that needs replacing, I'm assuming you're asking if this new Badger 1 will install similarly to your original one. Am I correct? \n",
      "\n",
      "If that's the case, the good news is that the installation process should be similar. However, to ensure a smooth and hassle-free installation, I would recommend checking the product manual or instructions that come with the new Badger 1. If you're unsure or have any questions, feel free to ask, and I'll do my best to assist you!\n",
      "\n",
      "Let me know if there's anything else I can help with!\n"
     ]
    }
   ],
   "source": [
    "# Example input\n",
    "input_state = {\n",
    "    \"question\": \"I have a 9 year old Badger 1 that needs replacing, will this Badger 1 install just like the original one?\"\n",
    "}\n",
    "\n",
    "# Run the graph\n",
    "result = qa_graph.invoke(input_state)\n",
    "\n",
    "# Print the final answer\n",
    "print(\"Final Answer:\", result[\"final_answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99609567",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ba8423",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
