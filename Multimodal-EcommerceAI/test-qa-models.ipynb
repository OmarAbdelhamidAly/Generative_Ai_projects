{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b786885d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_24964\\4053878870.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  question_encoder.load_state_dict(torch.load(\"q_encoder_finetuned.pth\", map_location=device))\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_24964\\4053878870.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  answer_encoder.load_state_dict(torch.load(\"a_encoder_finetuned.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Question: How secure is it from bugs?\n",
      "‚úÖ Predicted Answer: I had no problems. Even if you leave a gap with the elastic, at worst I had a fly or two. Most was when I left the openings unzipped to go in and out. On damp ground at one camp ground I had a couple dozen crickets UNDER the tent when I took it down. They crunched when I walked on the tent floor, yuk! But unless I let them in, I did not wake up to any additional boarders.\n",
      "üéØ Ground Truth: I had no problems. Even if you leave a gap with the elastic, at worst I had a fly or two. Most was when I left the openings unzipped to go in and out. On damp ground at one camp ground I had a couple dozen crickets UNDER the tent when I took it down. They crunched when I walked on the tent floor, yuk! But unless I let them in, I did not wake up to any additional boarders.\n",
      "üìä Similarity Score: 11.9449\n",
      "\n",
      "üîç Question: does anyone use this blow dryer to diffuse their hair? if so, how are the results?\n",
      "‚úÖ Predicted Answer: I use it to diffuse. Works fine, but I prefer less \"blowing around\" of the hair. Too much wind, but once u get the control of the dryer it's fine. Not worth the money though.\n",
      "üéØ Ground Truth: I use it to diffuse. Works fine, but I prefer less \"blowing around\" of the hair. Too much wind, but once u get the control of the dryer it's fine. Not worth the money though.\n",
      "üìä Similarity Score: 26.7881\n",
      "\n",
      "üîç Question: is this expelier pressed and non gmo?\n",
      "‚úÖ Predicted Answer: I don't actually know for sure but we have bought from them on several occasions and have been very pleased with our product. It is called \"organic & non-gmo\" so I think that means no chemicals.\n",
      "üéØ Ground Truth: I can't answer to whether it's expeller pressed or not but it is extra virgin, so your guess is as good as mine. As for the GMO issue, I have not heard (so far) that any avocado tree has ever been genetically modified.\n",
      "üìä Similarity Score: 17.8260\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define DualEncoder class again\n",
    "class DualEncoder(nn.Module):\n",
    "    def __init__(self, model_name, output_dim=128):\n",
    "        super().__init__()\n",
    "        self.base_model = AutoModel.from_pretrained(model_name)\n",
    "        self.projection = nn.Linear(self.base_model.config.hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_token = output.last_hidden_state[:, 0]\n",
    "        return self.projection(cls_token)\n",
    "\n",
    "# Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Reload models\n",
    "question_encoder = DualEncoder(model_name).to(device)\n",
    "answer_encoder = DualEncoder(model_name).to(device)\n",
    "question_encoder.load_state_dict(torch.load(\"q_encoder_finetuned.pth\", map_location=device))\n",
    "answer_encoder.load_state_dict(torch.load(\"a_encoder_finetuned.pth\", map_location=device))\n",
    "question_encoder.eval()\n",
    "answer_encoder.eval()\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"single_qna.csv\")[['Question', 'Answer']].dropna()\n",
    "df = df.sample(n=1000, random_state=42).reset_index(drop=True)  # use a subset for testing\n",
    "\n",
    "# Helper function\n",
    "def get_embeddings(model, texts):\n",
    "    tokens = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=64)\n",
    "    if 'token_type_ids' in tokens:\n",
    "        del tokens['token_type_ids']\n",
    "    tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "    with torch.no_grad():\n",
    "        return model(**tokens)\n",
    "\n",
    "# Encode all answers\n",
    "answer_texts = df['Answer'].tolist()\n",
    "answer_embeddings = get_embeddings(answer_encoder, answer_texts)\n",
    "\n",
    "# Try a few random questions\n",
    "test_indices = [5, 100, 250]  # or use any other indices\n",
    "for idx in test_indices:\n",
    "    question = df.loc[idx, 'Question']\n",
    "    true_answer = df.loc[idx, 'Answer']\n",
    "    \n",
    "    # Get question embedding\n",
    "    q_emb = get_embeddings(question_encoder, [question])  # shape [1, 128]\n",
    "    \n",
    "    # Compute similarities\n",
    "    sims = torch.matmul(q_emb, answer_embeddings.T).squeeze()  # shape [N]\n",
    "    top_idx = sims.argmax().item()\n",
    "    \n",
    "    print(f\"\\nüîç Question: {question}\")\n",
    "    print(f\"‚úÖ Predicted Answer: {df.loc[top_idx, 'Answer']}\")\n",
    "    print(f\"üéØ Ground Truth: {true_answer}\")\n",
    "    print(f\"üìä Similarity Score: {sims[top_idx]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57136093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_24964\\43127725.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  question_encoder.load_state_dict(torch.load(\"q_encoder_finetuned.pth\", map_location=device))\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_24964\\43127725.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  answer_encoder.load_state_dict(torch.load(\"a_encoder_finetuned.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé Evaluation Results\n",
      "‚úÖ Top-1 Accuracy: 0.4830\n",
      "‚úÖ Top-5 Accuracy: 0.7640\n",
      "‚úÖ Mean Reciprocal Rank (MRR): 0.6068\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define DualEncoder class again\n",
    "class DualEncoder(nn.Module):\n",
    "    def __init__(self, model_name, output_dim=128):\n",
    "        super().__init__()\n",
    "        self.base_model = AutoModel.from_pretrained(model_name)\n",
    "        self.projection = nn.Linear(self.base_model.config.hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_token = output.last_hidden_state[:, 0]\n",
    "        return self.projection(cls_token)\n",
    "\n",
    "# Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Reload models\n",
    "question_encoder = DualEncoder(model_name).to(device)\n",
    "answer_encoder = DualEncoder(model_name).to(device)\n",
    "question_encoder.load_state_dict(torch.load(\"q_encoder_finetuned.pth\", map_location=device))\n",
    "answer_encoder.load_state_dict(torch.load(\"a_encoder_finetuned.pth\", map_location=device))\n",
    "question_encoder.eval()\n",
    "answer_encoder.eval()\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"single_qna.csv\")[['Question', 'Answer']].dropna()\n",
    "df = df.sample(n=1000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Embedding helper\n",
    "def get_embeddings(model, texts, batch_size=32):\n",
    "    all_embeddings = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            tokens = tokenizer(batch, padding=True, truncation=True, return_tensors='pt', max_length=64)\n",
    "            if 'token_type_ids' in tokens:\n",
    "                del tokens['token_type_ids']\n",
    "            tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "            embeddings = model(**tokens)\n",
    "            all_embeddings.append(embeddings.cpu())\n",
    "    return torch.cat(all_embeddings)\n",
    "\n",
    "# Get all embeddings\n",
    "question_embeddings = get_embeddings(question_encoder, df['Question'].tolist())  # [N, 128]\n",
    "answer_embeddings = get_embeddings(answer_encoder, df['Answer'].tolist())        # [N, 128]\n",
    "\n",
    "# L2 normalize\n",
    "q_emb_norm = torch.nn.functional.normalize(question_embeddings, p=2, dim=1)\n",
    "a_emb_norm = torch.nn.functional.normalize(answer_embeddings, p=2, dim=1)\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "similarity = torch.matmul(q_emb_norm, a_emb_norm.T).cpu().numpy()  # shape [N, N]\n",
    "\n",
    "# Evaluation metrics\n",
    "def top_k_accuracy(similarity, k):\n",
    "    top_k = np.argsort(-similarity, axis=1)[:, :k]\n",
    "    correct = sum([i in top_k[i] for i in range(len(top_k))])\n",
    "    return correct / len(top_k)\n",
    "\n",
    "def mean_reciprocal_rank(similarity):\n",
    "    ranks = []\n",
    "    for i in range(similarity.shape[0]):\n",
    "        sorted_indices = np.argsort(-similarity[i])\n",
    "        rank = np.where(sorted_indices == i)[0][0] + 1  # rank starts from 1\n",
    "        ranks.append(1.0 / rank)\n",
    "    return np.mean(ranks)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nüîé Evaluation Results\")\n",
    "print(f\"‚úÖ Top-1 Accuracy: {top_k_accuracy(similarity, 1):.4f}\")\n",
    "print(f\"‚úÖ Top-5 Accuracy: {top_k_accuracy(similarity, 5):.4f}\")\n",
    "print(f\"‚úÖ Mean Reciprocal Rank (MRR): {mean_reciprocal_rank(similarity):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5f9025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b8b3d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b737abd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Lenovo\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]c:\\Users\\Lenovo\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\transformers\\pipelines\\question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n",
      "  0%|          | 1/200 [00:00<02:04,  1.59it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:02<00:00, 89.53it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Evaluation Results:\n",
      "‚úÖ Exact Match (EM): 35.50\n",
      "‚úÖ F1 Score: 50.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import evaluate  # üëà use this instead of datasets\n",
    "\n",
    "# ‚úÖ Load model and tokenizer\n",
    "model_path = \"trainer_squad_qa_model\"\n",
    "tokenizer_path = \"tokenizer_squad_qa_model\"\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=model_path,\n",
    "    tokenizer=tokenizer_path,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# ‚úÖ Load data\n",
    "df = pd.read_csv(\"single_qna.csv\")[['Question', 'Answer']].dropna()\n",
    "df = df.iloc[100000:100200].reset_index(drop=True)  # 100 samples\n",
    "\n",
    "# ‚úÖ Load SQuAD evaluation metric\n",
    "squad_metric = evaluate.load(\"squad\")\n",
    "\n",
    "# ‚úÖ Evaluation loop\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "    question = df.loc[i, \"Question\"]\n",
    "    context = df.loc[i, \"Answer\"]\n",
    "    \n",
    "    try:\n",
    "        result = qa_pipeline({\n",
    "            \"question\": question,\n",
    "            \"context\": context\n",
    "        })\n",
    "        predicted_answer = result[\"answer\"]\n",
    "    except:\n",
    "        predicted_answer = \"\"\n",
    "\n",
    "    predictions.append({\n",
    "        \"id\": str(i),\n",
    "        \"prediction_text\": predicted_answer\n",
    "    })\n",
    "    \n",
    "    references.append({\n",
    "        \"id\": str(i),\n",
    "        \"answers\": {\n",
    "            \"answer_start\": [0],  # You can improve this if your GT has real spans\n",
    "            \"text\": [context]\n",
    "        }\n",
    "    })\n",
    "\n",
    "# ‚úÖ Compute metrics\n",
    "results = squad_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "# ‚úÖ Print results\n",
    "print(\"\\nüìä Evaluation Results:\")\n",
    "print(f\"‚úÖ Exact Match (EM): {results['exact_match']:.2f}\")\n",
    "print(f\"‚úÖ F1 Score: {results['f1']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb90a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Lenovo\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "c:\\Users\\Lenovo\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\transformers\\pipelines\\question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Sample QA from CSV at index 0\n",
      "üß† Question: Just want to reiterate on the intercom volume control, it can be manually turned up and can it go so loud that it would be too loud? I ask because I'm\n",
      "üìò Ground Truth: Yes it can easialy be adjusted. I found it too loud once and had to turn it down but it stays in the range where you've left it last.\n",
      "ü§ñ Predicted Answer: Yes\n",
      "‚úÖ Exact Match: 0.00\n",
      "‚úÖ F1 Score: 6.90\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "\n",
    "# ‚úÖ Load model and tokenizer\n",
    "model_path = \"trainer_squad_qa_model\"\n",
    "tokenizer_path = \"tokenizer_squad_qa_model\"\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=model_path,\n",
    "    tokenizer=tokenizer_path,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# ‚úÖ Load CSV and subset\n",
    "df = pd.read_csv(\"single_qna.csv\")[['Question', 'Answer']].dropna()\n",
    "df = df.iloc[100000:100200].reset_index(drop=True)\n",
    "\n",
    "# ‚úÖ Load SQuAD evaluation metric\n",
    "squad_metric = evaluate.load(\"squad\")\n",
    "\n",
    "# ‚úÖ Test a single QA pair before full evaluation\n",
    "def test_single_qa_pair_from_csv(index, df, qa_pipeline, metric):\n",
    "    question = df.loc[index, \"Question\"]\n",
    "    ground_truth = df.loc[index, \"Answer\"]\n",
    "    \n",
    "    try:\n",
    "        result = qa_pipeline({\n",
    "            \"question\": question,\n",
    "            \"context\": ground_truth\n",
    "        })\n",
    "        predicted = result[\"answer\"]\n",
    "    except Exception as e:\n",
    "        predicted = \"\"\n",
    "        print(f\"‚ö†Ô∏è Pipeline error at index {index}: {e}\")\n",
    "    \n",
    "    pred_format = {\"id\": \"0\", \"prediction_text\": predicted}\n",
    "    ref_format = {\"id\": \"0\", \"answers\": {\"answer_start\": [0], \"text\": [ground_truth]}}\n",
    "\n",
    "    results = metric.compute(predictions=[pred_format], references=[ref_format])\n",
    "    \n",
    "    print(f\"\\nüîç Sample QA from CSV at index {index}\")\n",
    "    print(f\"üß† Question: {question}\")\n",
    "    print(f\"üìò Ground Truth: {ground_truth}\")\n",
    "    print(f\"ü§ñ Predicted Answer: {predicted}\")\n",
    "    print(f\"‚úÖ Exact Match: {results['exact_match']:.2f}\")\n",
    "    print(f\"‚úÖ F1 Score: {results['f1']:.2f}\")\n",
    "\n",
    "# üîé Try a specific sample before full loop\n",
    "test_single_qa_pair_from_csv(0, df, qa_pipeline, squad_metric)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61019768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Running Predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]c:\\Users\\Lenovo\\anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\transformers\\pipelines\\question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:01<00:00, 128.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Evaluation Results:\n",
      "‚úÖ BLEU Score: 0.45\n",
      "‚úÖ ROUGE-1: 50.14\n",
      "‚úÖ ROUGE-2: 40.58\n",
      "‚úÖ ROUGE-L: 49.76\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import evaluate\n",
    "import nltk\n",
    "\n",
    "# Download nltk punkt for tokenization\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load BLEU and ROUGE metrics\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# Load your QA pipeline\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"trainer_squad_qa_model\",\n",
    "    tokenizer=\"tokenizer_squad_qa_model\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(\"single_qna.csv\")[['Question', 'Answer']].dropna()\n",
    "df = df.iloc[100000:100200].reset_index(drop=True)  # 200 examples for evaluation\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "print(\"\\nüîç Running Predictions...\")\n",
    "for i in tqdm(range(len(df))):\n",
    "    question = df.loc[i, \"Question\"]\n",
    "    context = df.loc[i, \"Answer\"]\n",
    "    \n",
    "    try:\n",
    "        output = qa_pipeline({\"question\": question, \"context\": context})\n",
    "        pred_answer = output['answer']\n",
    "    except Exception:\n",
    "        pred_answer = \"\"\n",
    "\n",
    "    predictions.append(pred_answer)\n",
    "    references.append(context)\n",
    "\n",
    "# BLEU and ROUGE expect list of references (as list of list of tokens)\n",
    "bleu_results = bleu.compute(predictions=predictions, references=[[ref] for ref in references])\n",
    "rouge_results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nüìä Evaluation Results:\")\n",
    "print(f\"‚úÖ BLEU Score: {bleu_results['bleu'] * 100:.2f}\")\n",
    "print(f\"‚úÖ ROUGE-1: {rouge_results['rouge1'] * 100:.2f}\")\n",
    "print(f\"‚úÖ ROUGE-2: {rouge_results['rouge2'] * 100:.2f}\")\n",
    "print(f\"‚úÖ ROUGE-L: {rouge_results['rougeL'] * 100:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83e0ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dd2cb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Trained Model on 5 Examples ---\n",
      "\n",
      "Example 1:\n",
      "  Question: What type of breaks does it have? Two operated by hands, or one hand and one pedal?\n",
      "  Ground Truth Answer: Hello, Front hand brake. Rear coaster-brake (pedal operated when user installed). Regards, BMW - AutoGoodParts\n",
      "  Generated Answer: Two operated by hands\n",
      "\n",
      "Example 2:\n",
      "  Question: What are the dimensions of the white spaces between the leaves? I want to add a label with words.\n",
      "  Ground Truth Answer: It is approximately 5 1/2 \" but it the label is with just the plain label maker that sticks on that won't work maybe get it engraved?\n",
      "  Generated Answer: 5 1/2 \"\n",
      "\n",
      "Example 3:\n",
      "  Question: Do these headlights come with the bulbs included ?\n",
      "  Ground Truth Answer: The headlight bulbs are included, but *not* the blinker bulbs. However this is not a problem. When you remove the old headlights, you disconnect the main wires at the back and you twist & pull the entire blinker bulb assembly (the bulb comes out with it). Pull the bulb out of the assembly & install in the new headlights. Note: leveling these is somewhat tricky. Do *not* use the horizontally-oriented leveling screw, only the one on the top.\n",
      "  Generated Answer: yes\n",
      "\n",
      "Example 4:\n",
      "  Question: 2003 honda accord automatic?\n",
      "  Ground Truth Answer: 97 honda accord manual\n",
      "  Generated Answer: \n",
      "\n",
      "Example 5:\n",
      "  Question: will they fit a 1996 toyota tacoma\n",
      "  Ground Truth Answer: Go to any online parts store...autozone or whatever...find out your replacement lens model # and it should match one of thee above. Likely will be an H6054 style....but its best to double check. Good luck Frank\n",
      "  Generated Answer: You're welcome.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load tokenizer\n",
    "trained_tokenizer = AutoTokenizer.from_pretrained(\"tokenizer_Generative_qa_model\")\n",
    "\n",
    "# Load the base FLAN-T5 model\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\").to(\"cuda\")\n",
    "\n",
    "# Resize the base model's token embeddings to match the tokenizer\n",
    "base_model.resize_token_embeddings(len(trained_tokenizer))\n",
    "\n",
    "# Load the LoRA adapters on top of the resized base model\n",
    "trained_model = PeftModel.from_pretrained(base_model, \"trainer_Generative_qa_model\").to(\"cuda\")\n",
    "\n",
    "def generate_answer(question, context, model, tokenizer):\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=64,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"single_qna.csv\")[['Question', 'Answer']].dropna()\n",
    "df = df.iloc[:110000].reset_index(drop=True)\n",
    "\n",
    "# Select 5 random examples from the test dataset for evaluation\n",
    "sample_test_df = df.iloc[100000:110000].sample(5, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n--- Evaluating Trained Model on 5 Examples ---\")\n",
    "for index, row in sample_test_df.iterrows():\n",
    "    question = row['Question']\n",
    "    ground_truth_answer = row['Answer']\n",
    "    generated_answer = generate_answer(question, ground_truth_answer, trained_model, trained_tokenizer)\n",
    "\n",
    "    print(f\"\\nExample {index + 1}:\")\n",
    "    print(f\"  Question: {question}\")\n",
    "    print(f\"  Ground Truth Answer: {ground_truth_answer}\")\n",
    "    print(f\"  Generated Answer: {generated_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca3f2622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Trained Model on 5 Examples ---\n",
      "\n",
      "Example 1:\n",
      "üß† Question: What type of breaks does it have? Two operated by hands, or one hand and one pedal?\n",
      "üìò Ground Truth Answer: Hello, Front hand brake. Rear coaster-brake (pedal operated when user installed). Regards, BMW - AutoGoodParts\n",
      "ü§ñ Generated Answer: Rear coaster-brake (pedal operated when user installed) and front hand brake\n",
      "\n",
      "Example 2:\n",
      "üß† Question: What are the dimensions of the white spaces between the leaves? I want to add a label with words.\n",
      "üìò Ground Truth Answer: It is approximately 5 1/2 \" but it the label is with just the plain label maker that sticks on that won't work maybe get it engraved?\n",
      "ü§ñ Generated Answer: 2\" x 2\" x 2\" x 2\" x 2\" x 2\" x 2\"\n",
      "\n",
      "Example 3:\n",
      "üß† Question: Do these headlights come with the bulbs included ?\n",
      "üìò Ground Truth Answer: The headlight bulbs are included, but *not* the blinker bulbs. However this is not a problem. When you remove the old headlights, you disconnect the main wires at the back and you twist & pull the entire blinker bulb assembly (the bulb comes out with it). Pull the bulb out of the assembly & install in the new headlights. Note: leveling these is somewhat tricky. Do *not* use the horizontally-oriented leveling screw, only the one on the top.\n",
      "ü§ñ Generated Answer: The bulbs are included, but not the blinker bulbs. However this is not a problem. When you remove the old headlights, you disconnect the main wires at the back and you twist & pull the entire blinker bulb assembly (the bulb comes out with it). Pull the bulb out of the assembly & install in the new headlights.\n",
      "\n",
      "Example 4:\n",
      "üß† Question: 2003 honda accord automatic?\n",
      "üìò Ground Truth Answer: 97 honda accord manual\n",
      "ü§ñ Generated Answer: Context: The 2003 Honda Accord was a mid-size sedan produced by Honda in the United States. It was the second generation of the Honda Accord and the first generation of the Honda Accord to be produced by Honda in the United States. The 2003 Honda Accord was manufactured by Honda in the United States.\n",
      "\n",
      "Example 5:\n",
      "üß† Question: will they fit a 1996 toyota tacoma\n",
      "üìò Ground Truth Answer: Go to any online parts store...autozone or whatever...find out your replacement lens model # and it should match one of thee above. Likely will be an H6054 style....but its best to double check. Good luck Frank\n",
      "ü§ñ Generated Answer: I'm not sure if that's what you're looking for or if it's just a question.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import pandas as pd\n",
    "\n",
    "# Load tokenizer\n",
    "trained_tokenizer = AutoTokenizer.from_pretrained(\"tokenizer_Generative_qa_model\")\n",
    "\n",
    "# Load base model and apply LoRA adapter\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\").to(\"cuda\")\n",
    "base_model.resize_token_embeddings(len(trained_tokenizer))\n",
    "trained_model = PeftModel.from_pretrained(base_model, \"trainer_Generative_qa_model\").to(\"cuda\")\n",
    "\n",
    "# Define generation function with better control\n",
    "def generate_answer(question, context, model, tokenizer):\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=128,             # üîº Increase max_length\n",
    "        min_length=20,              # ‚úÖ Encourage longer answers\n",
    "        num_beams=5,                # üîº Slightly better diversity\n",
    "        repetition_penalty=1.2,     # üîÑ Penalize repeats\n",
    "        length_penalty=1.0,         # ‚öñÔ∏è Balance between short and long\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer.strip()\n",
    "\n",
    "# Load and sample dataset\n",
    "df = pd.read_csv(\"single_qna.csv\")[['Question', 'Answer']].dropna()\n",
    "df = df.iloc[:110000].reset_index(drop=True)\n",
    "sample_test_df = df.iloc[100000:110000].sample(5, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\n--- Evaluating Trained Model on 5 Examples ---\")\n",
    "for index, row in sample_test_df.iterrows():\n",
    "    question = row['Question']\n",
    "    ground_truth_answer = row['Answer']\n",
    "    generated_answer = generate_answer(question, ground_truth_answer, trained_model, trained_tokenizer)\n",
    "\n",
    "    print(f\"\\nExample {index + 1}:\")\n",
    "    print(f\"üß† Question: {question}\")\n",
    "    print(f\"üìò Ground Truth Answer: {ground_truth_answer}\")\n",
    "    print(f\"ü§ñ Generated Answer: {generated_answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205c8361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b684cd5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df50f9f3",
   "metadata": {},
   "source": [
    "# get_top_k_answers_dual_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c779a73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_71184\\187008320.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  question_encoder.load_state_dict(torch.load(\"q_encoder_finetuned.pth\", map_location=device))\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_71184\\187008320.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  answer_encoder.load_state_dict(torch.load(\"a_encoder_finetuned.pth\", map_location=device))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define DualEncoder model\n",
    "class DualEncoder(nn.Module):\n",
    "    def __init__(self, model_name, output_dim=128):\n",
    "        super().__init__()\n",
    "        self.base_model = AutoModel.from_pretrained(model_name)\n",
    "        self.projection = nn.Linear(self.base_model.config.hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_token = output.last_hidden_state[:, 0]\n",
    "        return self.projection(cls_token)\n",
    "\n",
    "# Load model and tokenizer once\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load encoders and set to eval\n",
    "question_encoder = DualEncoder(model_name).to(device)\n",
    "answer_encoder = DualEncoder(model_name).to(device)\n",
    "question_encoder.load_state_dict(torch.load(\"q_encoder_finetuned.pth\", map_location=device))\n",
    "answer_encoder.load_state_dict(torch.load(\"a_encoder_finetuned.pth\", map_location=device))\n",
    "question_encoder.eval()\n",
    "answer_encoder.eval()\n",
    "\n",
    "# Load data and precompute answer embeddings\n",
    "df = pd.read_csv(\"single_qna.csv\")[['Question', 'Answer']].dropna()\n",
    "df = df.sample(n=1000, random_state=42).reset_index(drop=True)  # subset\n",
    "\n",
    "# Helper to get embeddings\n",
    "def get_embeddings(model, texts):\n",
    "    tokens = tokenizer(texts, padding=True, truncation=True, return_tensors='pt', max_length=64)\n",
    "    tokens.pop(\"token_type_ids\", None)\n",
    "    tokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "    with torch.no_grad():\n",
    "        return model(**tokens)\n",
    "\n",
    "# Precompute answer embeddings\n",
    "answer_texts = df['Answer'].tolist()\n",
    "answer_embeddings = get_embeddings(answer_encoder, answer_texts)  # shape [N, 128]\n",
    "\n",
    "# ‚úÖ Function: Get top K answers for a given question\n",
    "def get_top_k_answers_dual_encoder(question, top_k=3):\n",
    "    q_emb = get_embeddings(question_encoder, [question])  # shape [1, 128]\n",
    "    sims = torch.matmul(q_emb, answer_embeddings.T).squeeze()  # [N]\n",
    "    topk_scores, topk_indices = torch.topk(sims, k=top_k)\n",
    "\n",
    "    top_answers = []\n",
    "    for score, idx in zip(topk_scores.tolist(), topk_indices.tolist()):\n",
    "        top_answers.append({\n",
    "            \"answer\": df.loc[idx, 'Answer'],\n",
    "            \"score\": round(score, 4),\n",
    "            \"reference_question\": df.loc[idx, 'Question']\n",
    "        })\n",
    "    \n",
    "    return top_answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88306605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Selected Question from CSV (Index 3): Does this come with power cord and dishwasher hook up?\n",
      "\n",
      "üîù Rank 1:\n",
      "‚úÖ Answer: No, I don't believe so.\n",
      "üìä Score: 228.3117\n",
      "üìù From Original Question: Can this filter be used as an inline filter for refrigerator ice maker (no water dispenser)? I have been using the yellow twist and lock product specifically marketed for the purpose, but was told the green filter was more effective.\n",
      "\n",
      "üîù Rank 2:\n",
      "‚úÖ Answer: IT USES TWO LIGHT BULBS.\n",
      "üìä Score: 223.4112\n",
      "üìù From Original Question: how many light bulbs does it use\n",
      "\n",
      "üîù Rank 3:\n",
      "‚úÖ Answer: I measured 74 inches\n",
      "üìä Score: 217.8462\n",
      "üìù From Original Question: How much height does the trim kit add?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset (make sure this matches your environment)\n",
    "df = pd.read_csv(\"single_qna.csv\")[['Question', 'Answer']].dropna().reset_index(drop=True)\n",
    "\n",
    "# Select a question from the dataset (e.g., index 42)\n",
    "idx = 3\n",
    "sample_question = df.loc[idx, 'Question']\n",
    "print(f\"\\nüîç Selected Question from CSV (Index {idx}): {sample_question}\")\n",
    "\n",
    "# Call the dual encoder function\n",
    "results = get_top_k_answers_dual_encoder(sample_question)\n",
    "\n",
    "# Display results\n",
    "for i, item in enumerate(results, 1):\n",
    "    print(f\"\\nüîù Rank {i}:\")\n",
    "    print(f\"‚úÖ Answer: {item['answer']}\")\n",
    "    print(f\"üìä Score: {item['score']}\")\n",
    "    print(f\"üìù From Original Question: {item['reference_question']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dd1952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183c937d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e88566d6",
   "metadata": {},
   "source": [
    "# get-generated-answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e37b53",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdb86d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import pandas as pd\n",
    "\n",
    "# Load tokenizer and model once\n",
    "trained_tokenizer = AutoTokenizer.from_pretrained(\"tokenizer_Generative_qa_model\")\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\").to(\"cuda\")\n",
    "base_model.resize_token_embeddings(len(trained_tokenizer))\n",
    "trained_model = PeftModel.from_pretrained(base_model, \"trainer_Generative_qa_model\").to(\"cuda\")\n",
    "\n",
    "# Core generation function\n",
    "def generate_answer(question, context, model, tokenizer):\n",
    "    input_text = f\"question: {question} context: {context}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=128,\n",
    "        min_length=20,\n",
    "        num_beams=5,\n",
    "        repetition_penalty=1.2,\n",
    "        length_penalty=1.0,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return answer.strip()\n",
    "\n",
    "# ‚úÖ Function to try on natural question\n",
    "def answer_natural_question(question, context=None, fallback_csv=\"single_qna.csv\"):\n",
    "    if context is None:\n",
    "        # If no context is provided, get a similar one from CSV randomly (you can plug in retrieval here)\n",
    "        df = pd.read_csv(fallback_csv)[['Question', 'Answer']].dropna().sample(1, random_state=42)\n",
    "        context = df.iloc[0]['Answer']\n",
    "    \n",
    "    generated = generate_answer(question, context, trained_model, trained_tokenizer)\n",
    "\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"used_context\": context,\n",
    "        \"generated_answer\": generated\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3625c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Question: will they fit a 1996 toyota tacoma\n",
      "üìò Used Context: EM GEAR will use USPS shipping for this item. Thanks\n",
      "ü§ñ Generated Answer: No problem. Is there anything else I can help you with? Is there anything else I can help you with?\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the role of mitochondria in a cell?\"\n",
    "result = answer_natural_question(question)\n",
    "\n",
    "print(f\"\\nüß† Question: {result['question']}\")\n",
    "print(f\"üìò Used Context: {result['used_context']}\")\n",
    "print(f\"ü§ñ Generated Answer: {result['generated_answer']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d0fee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03263ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Sample QA from CSV at index 0\n",
      "üß† Question: Just want to reiterate on the intercom volume control, it can be manually turned up and can it go so loud that it would be too loud? I ask because I'm\n",
      "üìò Ground Truth: Yes it can easialy be adjusted. I found it too loud once and had to turn it down but it stays in the range where you've left it last.\n",
      "ü§ñ Predicted Answer: Yes\n",
      "üîé Confidence Score: 0.000\n",
      "‚úÖ Exact Match: 0.00\n",
      "‚úÖ F1 Score: 6.90\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_path = \"trainer_squad_qa_model\"\n",
    "tokenizer_path = \"tokenizer_squad_qa_model\"\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Device set to use cuda:{device if device>=0 else 'cpu'}\")\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=model_path,\n",
    "    tokenizer=tokenizer_path,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Load CSV and subset\n",
    "df = pd.read_csv(\"single_qna.csv\")[['Question', 'Answer']].dropna()\n",
    "df = df.iloc[100000:100200].reset_index(drop=True)\n",
    "\n",
    "# Load SQuAD evaluation metric\n",
    "squad_metric = evaluate.load(\"squad\")\n",
    "\n",
    "def test_single_qa_pair_from_csv(index, df, qa_pipeline, metric):\n",
    "    question = df.loc[index, \"Question\"]\n",
    "    ground_truth = df.loc[index, \"Answer\"]\n",
    "\n",
    "    # For better prediction, ideally supply a longer context, not just ground truth answer.\n",
    "    # Here, we use ground_truth as context as placeholder.\n",
    "    context = ground_truth  # <-- replace this with a longer context if available\n",
    "\n",
    "    try:\n",
    "        # Use keyword arguments (not deprecated list input)\n",
    "        result = qa_pipeline(question=question, context=context)\n",
    "        predicted = result.get(\"answer\", \"\")\n",
    "        score = result.get(\"score\", 0.0)\n",
    "    except Exception as e:\n",
    "        predicted = \"\"\n",
    "        score = 0.0\n",
    "        print(f\"‚ö†Ô∏è Pipeline error at index {index}: {e}\")\n",
    "\n",
    "    pred_format = {\"id\": str(index), \"prediction_text\": predicted}\n",
    "    ref_format = {\"id\": str(index), \"answers\": {\"answer_start\": [0], \"text\": [ground_truth]}}\n",
    "\n",
    "    results = metric.compute(predictions=[pred_format], references=[ref_format])\n",
    "\n",
    "    print(f\"\\nüîç Sample QA from CSV at index {index}\")\n",
    "    print(f\"üß† Question: {question}\")\n",
    "    print(f\"üìò Ground Truth: {ground_truth}\")\n",
    "    print(f\"ü§ñ Predicted Answer: {predicted}\")\n",
    "    print(f\"üîé Confidence Score: {score:.3f}\")\n",
    "    print(f\"‚úÖ Exact Match: {results['exact_match']:.2f}\")\n",
    "    print(f\"‚úÖ F1 Score: {results['f1']:.2f}\")\n",
    "\n",
    "# Try sample index 0\n",
    "test_single_qa_pair_from_csv(0, df, qa_pipeline, squad_metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9700548c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç QA Sample at index 0\n",
      "üß† Question: Just want to reiterate on the intercom volume control, it can be manually turned up and can it go so loud that it would be too loud? I ask because I'm\n",
      "üìò Ground Truth: Yes it can easialy be adjusted. I found it too loud once and had to turn it down but it stays in the range where you've left it last.\n",
      "ü§ñ Predicted Answer: Yes\n",
      "üîé Confidence Score: 0.000\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_path = \"trainer_squad_qa_model\"\n",
    "tokenizer_path = \"tokenizer_squad_qa_model\"\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(f\"Device set to use cuda:{device if device >= 0 else 'cpu'}\")\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=model_path,\n",
    "    tokenizer=tokenizer_path,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Load CSV and subset\n",
    "df = pd.read_csv(\"single_qna.csv\")[['Question', 'Answer']].dropna()\n",
    "df = df.iloc[100000:100200].reset_index(drop=True)\n",
    "\n",
    "def get_answer_from_csv(index, df, qa_pipeline):\n",
    "    question = df.loc[index, \"Question\"]\n",
    "    ground_truth = df.loc[index, \"Answer\"]\n",
    "\n",
    "    # IMPORTANT: replace this with a larger context related to the question if you have it!\n",
    "    context = ground_truth  # This is just the answer text, which causes short predictions.\n",
    "\n",
    "    try:\n",
    "        result = qa_pipeline(question=question, context=context)\n",
    "        predicted = result.get(\"answer\", \"\")\n",
    "        score = result.get(\"score\", 0.0)\n",
    "    except Exception as e:\n",
    "        predicted = \"\"\n",
    "        score = 0.0\n",
    "        print(f\"‚ö†Ô∏è Pipeline error at index {index}: {e}\")\n",
    "\n",
    "    print(f\"\\nüîç QA Sample at index {index}\")\n",
    "    print(f\"üß† Question: {question}\")\n",
    "    print(f\"üìò Ground Truth: {ground_truth}\")\n",
    "    print(f\"ü§ñ Predicted Answer: {predicted}\")\n",
    "    print(f\"üîé Confidence Score: {score:.3f}\")\n",
    "\n",
    "# Test sample index 0\n",
    "get_answer_from_csv(0, df, qa_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ca085e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Pipeline error at index 3: name 'context' is not defined\n",
      "\n",
      "üîç QA Sample at index 3\n",
      "üß† Question: what comes in the box\n",
      "üìò Ground Truth: Two Sena Bluetooth headsets, charger, windsocks. Go ahead and put the windsocks I. The mics. It will cut down on wind and give you better sound. We love our headsets.\n",
      "ü§ñ Predicted Answer: \n",
      "üîé Confidence Score: 0.000\n"
     ]
    }
   ],
   "source": [
    "def get_answer_from_csv(index, df, qa_pipeline):\n",
    "    question = df.loc[index, \"Question\"]\n",
    "    ground_truth = df.loc[index, \"Answer\"]\n",
    "\n",
    "    # Use question + answer as naive larger context\n",
    "    context = question + \". \" + ground_truth\n",
    "\n",
    "    try:\n",
    "        result = qa_pipeline(question=question, context=context)\n",
    "        predicted = result.get(\"answer\", \"\")\n",
    "        score = result.get(\"score\", 0.0)\n",
    "    except Exception as e:\n",
    "        predicted = \"\"\n",
    "        score = 0.0\n",
    "        print(f\"‚ö†Ô∏è Pipeline error at index {index}: {e}\")\n",
    "\n",
    "    print(f\"\\nüîç QA Sample at index {index}\")\n",
    "    print(f\"üß† Question: {question}\")\n",
    "    print(f\"üìò Ground Truth: {ground_truth}\")\n",
    "    print(f\"ü§ñ Predicted Answer: {predicted}\")\n",
    "    print(f\"üîé Confidence Score: {score:.3f}\")\n",
    "\n",
    "# Test sample index 0\n",
    "get_answer_from_csv(3, df, qa_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee74709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a738b665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
